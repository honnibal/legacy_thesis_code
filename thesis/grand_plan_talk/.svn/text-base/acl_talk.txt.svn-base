Our paper is about improving the analysis of specific linguistic phenomena in CCGbank. CCGbank is a treebank of Combinatory Categorial Grammar derivations that Julia Hockenmaier semi-automatically acquired from the Penn Treebank. Formalisms like CCG, HPSG, LFG and LTAG produce logical forms from their syntactic analyses. But, corpora acquired from the Penn Treebank don't always have analyses that produce good logical forms. This paper is about improving the interpretation of noun phrases in CCGbank. We hope the work will be of interest to researchers who use related formalisms.

We're interested in improving CCGbank because we've got two hypotheses --- which some of you probably disagree with. The first is that these lexicalised formalisms offer some advantages as languages for representing linguistic phenomena. I think it's useful to encapsulate a word's domain of locality in one sign. It makes writing a good grammar that's semantically transparent easier, so there's some utility in these formalisms. The second hypothesis is that it's easier to get linguistic information into a computer by producing annotated examples of your grammar, rather than writing it out by hand. This is why we're interested in improving a treebank for CCG, although the same applies to these other formalisms.

So. What have we done? Well, first we've pulled together some existing corrections to CCGbank, before building on them with novel changes. I've only got time to talk about one set of changes today, so I'm going to talk about how we've added sensitivity to nominal predicates to CCGbank. This bit was inspired by a talk Tracy King gave at GEAF last year, where she said that detecting nominal predicates greatly improved powerset's QA system, which uses a manually written LFG grammar. The other notable changes we've made distinguish restrictive and non-restrictive relatives, and analyse partitive constructions better. You can find out about these at our poster.

So. Nominal predicates. They're nouns with argument structure. These examples show how a predicate-argument configuration --- the predicate destroy, its agent Rome, and its patient Carthage --- can be configured as a tensed clause, or as a noun morphologically derived from the verb. The nominal predicate can map its arguments to multiple surface positions, and the mapping depends on the whole configuration --- the possessive in this one realises an agent, but when the second argument is added it realises a patient. This will make it difficult to have the prepositions determine the semantic role of the arguments. Instead, we're going to assign the head a subcategorisation frame that assigns roles to its arguments, just as we do for the verbal predicate.

In CCG, each word gets a lexical category that represents its subcategorisation frame. The categories are curried functions: here, the determiner is a function from a noun to a noun phrase. A function can return another function. So a transitive verb is a function from a noun phrase, to a function from a noun phrase to a sentence. In other words, we take an NP to the right, and we get from it a verb phrase that takes a subject, producing a sentence.

It's easy to make these derivations semantically aware. We can do this by specifying a logical form --- which I've abbreviated here to avoid introducing more syntax --- and coindexing its arguments to the arguments in the syntax. This allows the logical form to be built on the fly during the derivation.

So. How do we get the same logical form for a nominal predicate? Well, the first thing we want to do is represent the core arguments in the lexical category, so that we've got something to coindex. This saves us from having the arguments determine their own semantic role --- instead we can map semantic roles to argument positions. So we have destruction here subcategorise for two PPs, even though they're both syntactically optional. If we only have one PP we'll just assign destruction a different category.

The next thing we need is for destruction to have access to Carthage and Rome. This means that the PPs have to be headed by the NP argument. The rest of the derivation follows pretty simply: we just apply the arguments, to retrieve the semantic analysis here --- which you'll note matches the semantic analysis of the verbal realisation, Rome destroyed Carthage.

But what about this common case --- where a core argument is supplied by a possessive? We want to fill this patient slot here with Carthage. First, it's clear that we'll assign Carthage the category NP. And we know we'll probably want to bracket the clitic with the NP, and we definitely want to end up with an NP. We also want the possessive here to be the thing that gives us an NP, because it's a determiner. So, we get this. The missing category is thus just N/PP --- an N missing a PP argument. Now, it might seem strange that we type it PP, but this allows us to reuse the patient category from destruction of Carthage, which is nice. We just underspecify the preposition feature. It also means we generalise well to this analysis, where the possessive realises the agent, not the patient. All that's happened is, the outer argument is now this of PP, leaving the genitive to fill this agent slot. We'd have a difficult ambiguity here if we tried to determine the semantic role of the arguments in their categories, rather than have them assigned by the head. This way, everything gets its canonical category, and the argument reordering is driven by this function word, the possessive marker.

We can quantify the extent of the changes by asking what percentage of dependencies were left unchanged. After all of our changes, about 30% of dependencies in CCGbank were altered. So the parsing task has changed substantially, including all these new distinctions that take the parsing task towards semantic role labelling. We were interested in how easy it was to parse with the new corpus, so we trained the C&C parser to reproduce the new dependencies. These are the results on Section 23. As we expected, the new annotations are more difficult to reproduce. This is probably because of the extra information in the derivations. But maybe it was just that we introduced sparse data problems, because of all these extra categories. To investigate this, we evaluated the parser on just the dependencies that remained unchanged --- so that the numbers could be compared directly. The parser performed 0.8% worse on these shared dependencies, which were 70% of the total. This suggests that sparse data problems did account for some of the 3% drop in performance, but that most of it is best interpretted as the task becoming harder.


